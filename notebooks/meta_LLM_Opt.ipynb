{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1da8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67583d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (0.14.6)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (1.17.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d3646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json \n",
    "import time\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sys\n",
    "sys.path.insert(0,r\"C:Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/src\")\n",
    "from data.data_cleaning  import CleanData\n",
    "from data.data_preprocessing import DataPreprocessing\n",
    "from models.model import BiLSTMForecast\n",
    "from models.training import Trainer \n",
    "from pipelines.forecasting_pipeline import ForecastingPipeline\n",
    "import pandas as pd \n",
    "import re\n",
    "import os \n",
    "from meta_HPO.meta_knowledge_building import MetaKnowledgeBuilder\n",
    "from meta_HPO.llm_prompt import prompt_template\n",
    "from meta_HPO.LLM_Load import load_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0b432",
   "metadata": {},
   "source": [
    "# 1 .Meta-Knowledge Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9713aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user.IBRAHIM-IK-SZHE\\anaconda3\\envs\\HPO311\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best historical trials:\n",
      "{'run_id': 'trial_1', 'test_rmse': 1.1170297250018493, 'params': {'optimizer': 'adamax', 'epochs': '17', 'num_layers': '1', 'batch_size': '32', 'hidden_size': '126', 'lr': '0.0005182120604022333', 'horizon': '4', 'dropout': '0.0', 'input_dim': '4', 'model_name': 'Bi-LSTM', 'seed': '42', 'lag': '58'}}\n",
      "{'run_id': 'trial_2', 'test_rmse': 1.9278644352798011, 'params': {'optimizer': 'adamw', 'epochs': '6', 'num_layers': '1', 'batch_size': '32', 'hidden_size': '81', 'lr': '0.0003623165345530821', 'horizon': '4', 'dropout': '0.0', 'input_dim': '4', 'model_name': 'Bi-LSTM', 'seed': '42', 'lag': '38'}}\n",
      "{'run_id': 'trial_3', 'test_rmse': 2.7698660920690417, 'params': {'optimizer': 'adagrad', 'epochs': '6', 'num_layers': '3', 'batch_size': '128', 'hidden_size': '77', 'lr': '0.004802430658713304', 'horizon': '4', 'dropout': '0.3435927279521027', 'input_dim': '4', 'model_name': 'Bi-LSTM', 'seed': '42', 'lag': '49'}}\n",
      "‚úÖ Dataset regime:\n",
      "{'temporal_dependence': 'strong', 'stationarity': 'mostly stationary', 'trend': 'moderate', 'seasonality': 'strong', 'volatility': 'moderate', 'noise_level': 'low'}\n",
      "\n",
      "‚úÖ Aggregated meta-features:\n",
      "{'temporal_dependence_mean_acf1': 0.9828705509299869, 'stationarity_ratio': 1.0, 'avg_trend_strength': None, 'avg_seasonality_strength': None, 'avg_volatility_cv': 0.4885260253901742, 'avg_nonlinearity_proxy': 0.7810950068734697, 'avg_outlier_ratio': 0.01757732194252821}\n"
     ]
    }
   ],
   "source": [
    "#-----------test forecasting Pipelien----------\n",
    "# load Clean train/ test data\n",
    "Target_Cols = ['T (degC)', 'rh (%)', 'p (mbar)', 'wv (m/s)']\n",
    "\n",
    "\n",
    "clean_train = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/pape_project/hyperopt-llm-project/data/clean_data/clean_train.csv',\n",
    "parse_dates=[\"datetime\"],\n",
    "index_col=\"datetime\"\n",
    ")\n",
    "\n",
    "# MLflow experiment name (must already exist)\n",
    "EXPERIMENT_NAME = \"Bayesian_Optimization\"\n",
    "meta_builder= MetaKnowledgeBuilder(clean_train=clean_train,experiment_name=EXPERIMENT_NAME,target_cols=Target_Cols)\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "best_trials = meta_builder.create_meta_trials(top_k=top_k)\n",
    "\n",
    "print(\"‚úÖ Best historical trials:\")\n",
    "for t in best_trials:\n",
    "    print(t)\n",
    "\n",
    "meta_features = meta_builder.meta_features_creation()\n",
    "\n",
    "print(\"‚úÖ Dataset regime:\")\n",
    "print(meta_features[\"dataset_regime\"])\n",
    "\n",
    "print(\"\\n‚úÖ Aggregated meta-features:\")\n",
    "print(meta_features[\"aggregated_meta\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d184a",
   "metadata": {},
   "source": [
    "# Meta-knoweldge Extraction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb63ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "#  1.Model Architecture Description\n",
    "# ==================================================\n",
    "model_description = \"\"\"\n",
    "Bidirectional LSTM for multivariate forecasting:\n",
    "- Input: multivariate sequence\n",
    "- hidden_size: LSTM units\n",
    "- num_layers: stacked layers\n",
    "- dropout applied between layers\n",
    "\"\"\"\n",
    "# ==================================================\n",
    "#  1.extract meta-initila trials and meta_knowledge\n",
    "# ==================================================\n",
    "with open(\"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/meta_data/run_data.json\") as f:\n",
    "    best_trials = json.load(f)\n",
    "historical_trials = [t[\"params\"] for t in best_trials]\n",
    "\n",
    "with open(\"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/meta_data/meta_features.json\") as f:\n",
    "    meta_features = json.load(f)\n",
    "\n",
    "raw_features_summary = json.dumps(meta_features[\"raw_meta_features\"], indent=2)\n",
    "summary_meta_fetaures = json.dumps(meta_features[\"dataset_regime\"], indent=2)\n",
    "#--------------------------------\n",
    " # compute Traget Rmse \n",
    "#---------------------------------\n",
    "target_rmse = meta_builder.compute_target_rmse(best_trials, epsilon=0.08)\n",
    "#--------------------------------\n",
    " # Shots id train data \n",
    "#---------------------------------\n",
    "dataset_sample = json.dumps(clean_train.tail(30).to_dict(orient=\"records\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c626c57",
   "metadata": {},
   "source": [
    "# Load Open Source LLM using Ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c211416",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def _ollama_installed():\n",
    "    return shutil.which(\"ollama\") is not None\n",
    "\n",
    "\n",
    "def _ollama_model_exists(model_name: str):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"ollama\", \"list\"], text=True)\n",
    "        return any(model_name in line for line in out.splitlines())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _ollama_pull(model_name: str):\n",
    "    print(f\"‚¨áÔ∏è Pulling Ollama model: {model_name}\")\n",
    "    subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "\n",
    "\n",
    "def load_llm(\n",
    "    backend: str = \"ollama\",\n",
    "    model_name: str = \"qwen2:3b\",\n",
    "    temperature: float = 0.2,\n",
    "    warn_large: bool = True\n",
    "):\n",
    "    backend = backend.lower()\n",
    "\n",
    "    # --------------------------\n",
    "    # Ollama (BEST for CPU)\n",
    "    # --------------------------\n",
    "    if backend == \"ollama\":\n",
    "        if not _ollama_installed():\n",
    "            raise RuntimeError(\n",
    "                \"‚ùå Ollama not installed. Install from: https://ollama.com\"\n",
    "            )\n",
    "\n",
    "        if not _ollama_model_exists(model_name):\n",
    "            _ollama_pull(model_name)\n",
    "\n",
    "        if warn_large and any(x in model_name for x in [\"13b\", \"70b\"]):\n",
    "            print(\"‚ö†Ô∏è WARNING: This model may be too large for CPU.\")\n",
    "\n",
    "        from langchain_community.llms import Ollama\n",
    "        return Ollama(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            num_ctx=4096,\n",
    "            num_thread=4   # adjust based on your CPU cores\n",
    "        )\n",
    "\n",
    "    # --------------------------\n",
    "    # HuggingFace (CPU fallback)\n",
    "    # --------------------------\n",
    "    elif backend == \"hf\":\n",
    "        from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "        if warn_large and any(x in model_name.lower() for x in [\"13b\", \"70b\"]):\n",
    "            print(\"‚ö†Ô∏è WARNING: Large HF model on CPU will be VERY slow.\")\n",
    "\n",
    "        return HuggingFaceHub(\n",
    "            repo_id=model_name,\n",
    "            model_kwargs={\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 512\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backend: ollama | openai | hf\")'''\n",
    "\n",
    "\n",
    "#Load LLama\n",
    "llm = load_llm(\n",
    "    backend=\"ollama\",\n",
    "    model_name=\"llama3:8b\",  # medium-small reasoning model\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9b8cb",
   "metadata": {},
   "source": [
    "# 3. Prompt Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d959d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"model_description\",\n",
    "        \"num_features\",\n",
    "        \"dataset_meta_summary\",\n",
    "        \"raw_features_summary\",\n",
    "        \"dataset_sample\",\n",
    "        \"historical_trials\",\n",
    "        \"target_rmse\",\n",
    "        \"current_best_rmse\"\n",
    "    ],\n",
    "    template=\"\"\"\n",
    "You are an expert hyperparameter optimization agent specialized in\n",
    "MULTIVARIATE TIME SERIES FORECASTING using a BIDIRECTIONAL LSTM (BiLSTM).\n",
    "\n",
    "Your SOLE OBJECTIVE is to MINIMIZE TEST RMSE.\n",
    "You must suggest new configurations that IMPROVE upon the CURRENT BEST RMSE.\n",
    "Treat historical_trials as a TABU LIST ‚Äî NEVER repeat them.\n",
    "\n",
    "==================================================\n",
    "OBJECTIVE\n",
    "==================================================\n",
    "Current best RMSE: {current_best_rmse}\n",
    "Target RMSE: < {target_rmse}\n",
    "Goal: Each suggestion must move the search toward LOWER RMSE.\n",
    "\n",
    "==================================================\n",
    "DATASET CONTEXT & META-FEATURES\n",
    "==================================================\n",
    "Dataset type: Multivariate time series regression\n",
    "Number of input variables: {num_features}\n",
    "\n",
    "Dataset meta-features:\n",
    "{dataset_meta_summary}\n",
    "\n",
    "Use meta-features to reason about hyperparameters:\n",
    "- Seasonality ‚Üí lag\n",
    "- Autocorrelation ‚Üí lag, hidden_size\n",
    "- Cross-variable interactions ‚Üí hidden_size, num_layers\n",
    "- Noise level ‚Üí dropout, batch_size\n",
    "- Stationarity & trend ‚Üí depth, learning rate\n",
    "- Dataset size ‚Üí model capacity, epochs\n",
    "\n",
    "Raw feature summaries (for context only):\n",
    "{raw_features_summary}\n",
    "\n",
    "Small data sample (qualitative insight only ‚Äî do NOT overfit):\n",
    "{dataset_sample}\n",
    "\n",
    "==================================================\n",
    "MODEL CONTEXT\n",
    "==================================================\n",
    "Architecture:\n",
    "- Bidirectional LSTM (capacity is effectively doubled)\n",
    "- Regression output\n",
    "\n",
    "Model description:\n",
    "{model_description}\n",
    "\n",
    "==================================================\n",
    "STRICT SEARCH SPACE\n",
    "==================================================\n",
    "lag ‚àà [12, 16, 24, 32]\n",
    "hidden_size ‚àà [24, 32, 48, 64]\n",
    "num_layers ‚àà [1, 2]\n",
    "dropout ‚àà [0.05, 0.1, 0.15, 0.2]\n",
    "lr ‚àà [0.0005, 0.001, 0.002]\n",
    "batch_size ‚àà [32, 64]\n",
    "epochs ‚àà [20, 30, 40]\n",
    "optimizer ‚àà [\"adam\", \"adamw\"]\n",
    "\n",
    "==================================================\n",
    "HISTORICAL TRIALS (TABU LIST)\n",
    "==================================================\n",
    "{historical_trials}\n",
    "\n",
    "Rules for this trial:\n",
    "1. You MUST NOT repeat any configuration from historical_trials.\n",
    "2. Change AT LEAST TWO hyperparameters from the last trial.\n",
    "3. At least ONE change MUST be from: dropout, lr, or epochs.\n",
    "4. Prioritize changes that are most likely to reduce RMSE based on dataset meta-features.\n",
    "5. Provide concise reasoning for why these changes are expected to improve RMSE.\n",
    "6. Confirm novelty of configuration before output.\n",
    "\n",
    "==================================================\n",
    "OUTPUT FORMAT (STRICT JSON)\n",
    "==================================================\n",
    "Output ONLY valid JSON. No markdown, no comments, no extra text.\n",
    "Return exactly this structure:\n",
    "\n",
    "{{\n",
    "  \"suggested_params\": {{\n",
    "    \"lag\": integer,\n",
    "    \"hidden_size\": integer,\n",
    "    \"num_layers\": integer,\n",
    "    \"dropout\": float,\n",
    "    \"lr\": float,\n",
    "    \"batch_size\": integer,\n",
    "    \"epochs\": integer,\n",
    "    \"optimizer\": \"string\"\n",
    "  }},\n",
    "  \"meta_feature_reasoning\": \"short explanation linking dataset characteristics to hyperparameter choices\",\n",
    "  \"non_repetition_check\": \"confirmation that this configuration is novel and not in historical_trials\",\n",
    "  \"expected_improvement\": \"short estimate of why RMSE should improve\"\n",
    "}}\n",
    "No Extrat Data\n",
    "\"\"\"\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11282f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Pulling Ollama model: phi3:mini\n"
     ]
    }
   ],
   "source": [
    "llm_phi3mini = load_llm(\n",
    "    backend=\"ollama\",\n",
    "    model_name=\"phi3:mini\",\n",
    "    temperature=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284e0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define llm chain \n",
    "suggest_chain = LLMChain(\n",
    "    llm=llm_phi3mini,\n",
    "    prompt=prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce4b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46b7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = \"Bi-LSTM\"\n",
    "experiment_name =\"LLM-AutoOpt\"\n",
    "TARGET_COLS =['T (degC)', 'rh (%)', 'p (mbar)', 'wv (m/s)']\n",
    "clean_train = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_train.csv',\n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "clean_test = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_test.csv', \n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "\n",
    "# define ForecastingPipeline\n",
    "pipeline = ForecastingPipeline(\n",
    "    clean_train=clean_train,\n",
    "    clean_test=clean_test,\n",
    "    target_cols=TARGET_COLS,\n",
    "    experiment_name=experiment_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3528ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tools\n",
    "def run_bilstm(params: dict) -> float:\n",
    "    \"\"\"\n",
    "    Runs Bi-LSTM and returns RMSE\n",
    "    \"\"\"\n",
    "    results = pipeline.run(\"Bi-LSTM\", params)\n",
    "    return float(results[\"rmse\"])\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "#Flexiable \n",
    "def extract_params_and_reasoning(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Flexible extractor for suggested_params and reasoning from LLM output.\n",
    "    Works whether JSON is inside ```json``` fences or plain text.\n",
    "    \"\"\"\n",
    "    # ---------------------------\n",
    "    # 1Ô∏è‚É£ Extract JSON-like substring\n",
    "    # ---------------------------\n",
    "    # Try to find a JSON block inside ```json``` first\n",
    "    json_match = re.search(r\"```json(.*?)```\", text, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1).strip()\n",
    "    else:\n",
    "        # Otherwise, find first {...} block in the text\n",
    "        json_match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No JSON object found in text\")\n",
    "        json_str = json_match.group(0)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2Ô∏è‚É£ Load JSON safely\n",
    "    # ---------------------------\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON detected: {e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3Ô∏è‚É£ Extract keys\n",
    "    # ---------------------------\n",
    "    if \"suggested_params\" not in data:\n",
    "        raise KeyError(\"'suggested_params' not found in JSON\")\n",
    "\n",
    "    reasoning = data.get(\"meta_feature_reasoning\")  # may be None if missing\n",
    "\n",
    "    return {\n",
    "        \"suggested_params\": data[\"suggested_params\"],\n",
    "        \"meta_feature_reasoning\": reasoning\n",
    "    }\n",
    "\n",
    "# record the trials \n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def record_trials(\n",
    "    params,\n",
    "    rmse,\n",
    "    reasoning,\n",
    "     llm_name,\n",
    "    base_dir=\"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/LLM_trials_memory\",  \n",
    "):\n",
    "    record = {\n",
    "        \"params\": params,\n",
    "        \"rmse\": rmse,\n",
    "        \"reasoning\": reasoning\n",
    "    }\n",
    "    filename = f\"trials_history_{llm_name}.json\"\n",
    "    base_dir = Path(base_dir)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)   # ensure folder exists\n",
    "\n",
    "    file_path = base_dir / filename               # real file path\n",
    "\n",
    "    # Load existing history\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []   # corrupted/empty file fallback\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    # Append new record\n",
    "    data.append(record)\n",
    "\n",
    "    # Save back\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Trial recorded ‚Üí {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b23f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== üöÄ HPO Trial 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user.IBRAHIM-IK-SZHE\\anaconda3\\envs\\HPO311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 16,\n",
      "    \"hidden_size\": 48,\n",
      "    \"num_layers\": 2,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.002,\n",
      "    \"batch_size\": 64,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adamw\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"Increasing the lag to 16 and hidden size to 48 should capture more complex patterns in weather data. A bidirectional LSTM with two layers can better learn from both past and future contexts, which is beneficial for capturing seasonality.\",\n",
      "  \"non_repetition_check\": true,\n",
      "    \"expected_improvement\": \"Based on the dataset's longer-term dependencies and non-stationarity in weather patterns, these changes should enhance learning of temporal features leading to a lower RMSE.\"\n",
      "}\n",
      "Reasoning Increasing the lag to 16 and hidden size to 48 should capture more complex patterns in weather data. A bidirectional LSTM with two layers can better learn from both past and future contexts, which is beneficial for capturing seasonality.\n",
      "Epoch [1/30] Loss: 0.030062 | RMSE: 0.173175\n",
      "Epoch [2/30] Loss: 0.013835 | RMSE: 0.117385\n",
      "Epoch [3/30] Loss: 0.008891 | RMSE: 0.094093\n",
      "Epoch [4/30] Loss: 0.007741 | RMSE: 0.087905\n",
      "Epoch [5/30] Loss: 0.007820 | RMSE: 0.088454\n",
      "Epoch [6/30] Loss: 0.007445 | RMSE: 0.086287\n",
      "Epoch [7/30] Loss: 0.006779 | RMSE: 0.082368\n",
      "Epoch [8/30] Loss: 0.006044 | RMSE: 0.077747\n",
      "Epoch [9/30] Loss: 0.005220 | RMSE: 0.072202\n",
      "Epoch [10/30] Loss: 0.004406 | RMSE: 0.066386\n",
      "Epoch [11/30] Loss: 0.003582 | RMSE: 0.059868\n",
      "Epoch [12/30] Loss: 0.002780 | RMSE: 0.052753\n",
      "Epoch [13/30] Loss: 0.002126 | RMSE: 0.046119\n",
      "Epoch [14/30] Loss: 0.001758 | RMSE: 0.041901\n",
      "Epoch [15/30] Loss: 0.001380 | RMSE: 0.037132\n",
      "Epoch [16/30] Loss: 0.001324 | RMSE: 0.036374\n",
      "Epoch [17/30] Loss: 0.001518 | RMSE: 0.038954\n",
      "Epoch [18/30] Loss: 0.002689 | RMSE: 0.051683\n",
      "Epoch [19/30] Loss: 0.003004 | RMSE: 0.054721\n",
      "Epoch [20/30] Loss: 0.001800 | RMSE: 0.042436\n",
      "Epoch [21/30] Loss: 0.001471 | RMSE: 0.038352\n",
      "Epoch [22/30] Loss: 0.001936 | RMSE: 0.043960\n",
      "Epoch [23/30] Loss: 0.001893 | RMSE: 0.043538\n",
      "Epoch [24/30] Loss: 0.001590 | RMSE: 0.039888\n",
      "Epoch [25/30] Loss: 0.001631 | RMSE: 0.040318\n",
      "Epoch [26/30] Loss: 0.001524 | RMSE: 0.039030\n",
      "Epoch [27/30] Loss: 0.001465 | RMSE: 0.038289\n",
      "Epoch [28/30] Loss: 0.001548 | RMSE: 0.039369\n",
      "Epoch [29/30] Loss: 0.001544 | RMSE: 0.039337\n",
      "Epoch [30/30] Loss: 0.001433 | RMSE: 0.037878\n",
      "Test RMSE: 2.0428 | SMAPE: 13.39%\n",
      "üìâ RMSE: 2.0428047882551583\n",
      "\n",
      "===== üöÄ HPO Trial 2 =====\n",
      "üîß Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 12,\n",
      "    \"hidden_size\": 48,\n",
      "    \"num_layers\": 2,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.002,\n",
      "    \"batch_size\": 64,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adamw\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"Increasing the lag to a higher value within our search space could capture more extended temporal dependencies in weather patterns. A larger hidden size and additional LSTM layer should improve model capacity without overfitting, given that we have sufficient data for training.\",\n",
      "  \"non_repetition_check\": true,\n",
      "    \"expected_improvement\": \"The increased lag will allow the network to learn from a broader range of historical weather patterns. The larger hidden size and additional layer should enhance learning capacity while dropout helps prevent overfitting on this specific dataset's complexity.\"\n",
      "}\n",
      "Reasoning Increasing the lag to a higher value within our search space could capture more extended temporal dependencies in weather patterns. A larger hidden size and additional LSTM layer should improve model capacity without overfitting, given that we have sufficient data for training.\n",
      "Epoch [1/30] Loss: 0.030760 | RMSE: 0.175286\n",
      "Epoch [2/30] Loss: 0.015656 | RMSE: 0.124877\n",
      "Epoch [3/30] Loss: 0.009681 | RMSE: 0.098029\n",
      "Epoch [4/30] Loss: 0.007886 | RMSE: 0.088393\n",
      "Epoch [5/30] Loss: 0.007766 | RMSE: 0.087735\n",
      "Epoch [6/30] Loss: 0.007690 | RMSE: 0.087277\n",
      "Epoch [7/30] Loss: 0.007395 | RMSE: 0.085661\n",
      "Epoch [8/30] Loss: 0.006909 | RMSE: 0.082754\n",
      "Epoch [9/30] Loss: 0.006174 | RMSE: 0.078121\n",
      "Epoch [10/30] Loss: 0.005375 | RMSE: 0.072810\n",
      "Epoch [11/30] Loss: 0.004535 | RMSE: 0.066763\n",
      "Epoch [12/30] Loss: 0.003502 | RMSE: 0.058415\n",
      "Epoch [13/30] Loss: 0.002670 | RMSE: 0.050805\n",
      "Epoch [14/30] Loss: 0.001964 | RMSE: 0.043258\n",
      "Epoch [15/30] Loss: 0.001630 | RMSE: 0.039314\n",
      "Epoch [16/30] Loss: 0.001630 | RMSE: 0.039235\n",
      "Epoch [17/30] Loss: 0.001735 | RMSE: 0.040630\n",
      "Epoch [18/30] Loss: 0.002015 | RMSE: 0.043769\n",
      "Epoch [19/30] Loss: 0.002027 | RMSE: 0.044063\n",
      "Epoch [20/30] Loss: 0.001675 | RMSE: 0.039778\n",
      "Epoch [21/30] Loss: 0.001567 | RMSE: 0.038394\n",
      "Epoch [22/30] Loss: 0.001694 | RMSE: 0.039939\n",
      "Epoch [23/30] Loss: 0.001901 | RMSE: 0.042484\n",
      "Epoch [24/30] Loss: 0.001784 | RMSE: 0.041116\n",
      "Epoch [25/30] Loss: 0.001502 | RMSE: 0.037494\n",
      "Epoch [26/30] Loss: 0.001532 | RMSE: 0.038078\n",
      "Epoch [27/30] Loss: 0.001589 | RMSE: 0.038709\n",
      "Epoch [28/30] Loss: 0.001680 | RMSE: 0.039751\n",
      "Epoch [29/30] Loss: 0.001504 | RMSE: 0.037649\n",
      "Epoch [30/30] Loss: 0.001507 | RMSE: 0.037489\n",
      "Test RMSE: 2.5619 | SMAPE: 14.91%\n",
      "üìâ RMSE: 2.5618853489704936\n",
      "\n",
      "===== üöÄ HPO Trial 3 =====\n",
      "üîß Suggested JSON: ```json\n",
      "{\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 24,\n",
      "    \"hidden_size\": 64,\n",
      "    \"num_layers\": 2,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.002,\n",
      "    \"batch_size\": 64,\n",
      "    \"epochs\": 40,\n",
      "    \"optimizer\": \"adamw\",\n",
      "    \"input_dim\": 4\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"Increasing the lag to 24 hours and using a larger hidden size of 64 LSTM units may capture more complex temporal dependencies, while increasing epochs should allow for better learning. A higher batch size can improve training stability.\",\n",
      "  \"non_repetition_check\": true,\n",
      "  \"expected_improvement\": \"The increased lag and additional layer are expected to provide the model with a broader context of data which could lead to improved RMSE due to more accurate long-term forecasting. A higher learning rate might accelerate convergence while larger batch size can improve training stability, potentially leading to better performance.\"\n",
      "} \n",
      "```\n",
      "Reasoning Increasing the lag to 24 hours and using a larger hidden size of 64 LSTM units may capture more complex temporal dependencies, while increasing epochs should allow for better learning. A higher batch size can improve training stability.\n",
      "Epoch [1/40] Loss: 0.030730 | RMSE: 0.175322\n",
      "Epoch [2/40] Loss: 0.014605 | RMSE: 0.120866\n",
      "Epoch [3/40] Loss: 0.010022 | RMSE: 0.100122\n",
      "Epoch [4/40] Loss: 0.008484 | RMSE: 0.092117\n",
      "Epoch [5/40] Loss: 0.008300 | RMSE: 0.091112\n",
      "Epoch [6/40] Loss: 0.008253 | RMSE: 0.090854\n",
      "Epoch [7/40] Loss: 0.007911 | RMSE: 0.088950\n",
      "Epoch [8/40] Loss: 0.007399 | RMSE: 0.086021\n",
      "Epoch [9/40] Loss: 0.006740 | RMSE: 0.082101\n",
      "Epoch [10/40] Loss: 0.006112 | RMSE: 0.078186\n",
      "Epoch [11/40] Loss: 0.005380 | RMSE: 0.073353\n",
      "Epoch [12/40] Loss: 0.004513 | RMSE: 0.067184\n",
      "Epoch [13/40] Loss: 0.003555 | RMSE: 0.059620\n",
      "Epoch [14/40] Loss: 0.002641 | RMSE: 0.051385\n",
      "Epoch [15/40] Loss: 0.001998 | RMSE: 0.044690\n",
      "Epoch [16/40] Loss: 0.001501 | RMSE: 0.038728\n",
      "Epoch [17/40] Loss: 0.001167 | RMSE: 0.034152\n",
      "Epoch [18/40] Loss: 0.001019 | RMSE: 0.031902\n",
      "Epoch [19/40] Loss: 0.000971 | RMSE: 0.031144\n",
      "Epoch [20/40] Loss: 0.000964 | RMSE: 0.031036\n",
      "Epoch [21/40] Loss: 0.000925 | RMSE: 0.030394\n",
      "Epoch [22/40] Loss: 0.000953 | RMSE: 0.030853\n",
      "Epoch [23/40] Loss: 0.000887 | RMSE: 0.029768\n",
      "Epoch [24/40] Loss: 0.000959 | RMSE: 0.030952\n",
      "Epoch [25/40] Loss: 0.001016 | RMSE: 0.031859\n",
      "Epoch [26/40] Loss: 0.001290 | RMSE: 0.035911\n",
      "Epoch [27/40] Loss: 0.002613 | RMSE: 0.051116\n",
      "Epoch [28/40] Loss: 0.004391 | RMSE: 0.066265\n",
      "Epoch [29/40] Loss: 0.004981 | RMSE: 0.070581\n",
      "Epoch [30/40] Loss: 0.004292 | RMSE: 0.065508\n",
      "Epoch [31/40] Loss: 0.002370 | RMSE: 0.048675\n",
      "Epoch [32/40] Loss: 0.001620 | RMSE: 0.040232\n",
      "Epoch [33/40] Loss: 0.001230 | RMSE: 0.035056\n",
      "Epoch [34/40] Loss: 0.001107 | RMSE: 0.033255\n",
      "Epoch [35/40] Loss: 0.001168 | RMSE: 0.034158\n",
      "Epoch [36/40] Loss: 0.001371 | RMSE: 0.037014\n",
      "Epoch [37/40] Loss: 0.001784 | RMSE: 0.042223\n",
      "Epoch [38/40] Loss: 0.001767 | RMSE: 0.042017\n",
      "Epoch [39/40] Loss: 0.001619 | RMSE: 0.040220\n",
      "Epoch [40/40] Loss: 0.001502 | RMSE: 0.038734\n",
      "Test RMSE: 2.6661 | SMAPE: 11.01%\n",
      "üìâ RMSE: 2.666056352705715\n"
     ]
    }
   ],
   "source": [
    "  # 1Ô∏è‚É£ Ask LLM for new configuration\n",
    "best_rmse = 1.11\n",
    "\n",
    "max_trials = 3\n",
    "\n",
    "for i in range(max_trials):\n",
    "    print(f\"\\n===== üöÄ HPO Trial {i+1} =====\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Ask LLM for new configuration\n",
    "    config_json = suggest_chain.run(\n",
    "        model_description =  model_description,\n",
    "        num_features = 4,\n",
    "        dataset_meta_summary =  dataset_sample ,\n",
    "        raw_features_summary = raw_features_summary,\n",
    "        dataset_sample = dataset_sample,\n",
    "        historical_trials=json.dumps(historical_trials, indent=2),\n",
    "        current_best_rmse=best_rmse,\n",
    "        target_rmse=target_rmse\n",
    "        )\n",
    "\n",
    "    print(\"üîß Suggested JSON:\", config_json)\n",
    "    parsing=  extract_params_and_reasoning(config_json)\n",
    "    print(\"Reasoning\",parsing[\"meta_feature_reasoning\"])\n",
    "    # 3Ô∏è‚É£ Run model\n",
    "    rmse = run_bilstm(parsing[\"suggested_params\"])\n",
    "    print(\"üìâ RMSE:\", rmse)\n",
    "\n",
    "    # 4Ô∏è‚É£ Update best\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        historical_trials.append(parsing[\"suggested_params\"])\n",
    "        record_trials(parsing[\"suggested_params\"], best_rmse, parsing[\"suggested_params\"])\n",
    "        print(\"‚úÖ New best RMSE!\")\n",
    "\n",
    "    # 5Ô∏è‚É£ Early stopping\n",
    "    if best_rmse <= target_rmse:\n",
    "        print(\"üéØ Target RMSE reached!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e986f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         ID              SIZE      MODIFIED    \n",
      "llama3:8b    365c0bd3c000    4.7 GB    2 hours ago    \n"
     ]
    }
   ],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a21b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen 3B safely via Ollama\n",
    "llm = load_llm(\n",
    "    backend=\"ollama\",\n",
    "    model_name= \"qwen2.5:3b\",  # small CPU-friendly model\n",
    "    temperature=0.15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e3997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define llm chain \n",
    "suggest_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e34fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,r\"C:Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/src\")\n",
    "import pandas as pd \n",
    "from meta_HPO.meta_llm_tuning import LLMTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92b7531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_name =\"qwen2.5:3b\"\n",
    "name_model = \"Bi-LSTM\"\n",
    "experiment_name =\"LLM-AutoOpt\"\n",
    "TARGET_COLS =['T (degC)', 'rh (%)', 'p (mbar)', 'wv (m/s)']\n",
    "clean_train = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_train.csv',\n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "clean_test = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_test.csv', \n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "model_description = \"\"\"\n",
    "Bidirectional LSTM for multivariate forecasting:\n",
    "- Input: multivariate sequence\n",
    "- hidden_size: LSTM units\n",
    "- num_layers: stacked layers\n",
    "- dropout applied between layers\n",
    "\"\"\"\n",
    "llm_opt =  LLMTuning(name_model,experiment_name,clean_train,clean_test,TARGET_COLS,llm_name,model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_trials_data = \"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/meta_data/run_data.json\"\n",
    "meta_features_data = \"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/meta_data/meta_features.json\"\n",
    "historical_trials,dataset_sample,raw_features_summary,summary_meta_features,target_rmse,best_rmse = llm_opt.extract_metadata(meta_trials_data, meta_features_data, epsilon=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756330c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== HPO Trial 1 =====\n",
      "Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 24,\n",
      "    \"hidden_size\": 64,\n",
      "    \"num_layers\": 1,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.001,\n",
      "    \"batch_size\": 32,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adam\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"The dataset has a lag of 24, and reducing dropout to 0.15 from the previous trial's value of 0.3435927279521027 is expected to improve model generalization.\",\n",
      "  \"non_repetition_check\": \"This configuration is novel and not in historical_trials\",\n",
      "  \"expected_improvement\": \"Reducing dropout and increasing hidden_size are likely to reduce overfitting and capture more complex patterns, potentially improving RMSE.\"\n",
      "}\n",
      "Reasoning: The dataset has a lag of 24, and reducing dropout to 0.15 from the previous trial's value of 0.3435927279521027 is expected to improve model generalization.\n",
      "Epoch [1/30] Loss: 0.026265 | RMSE: 0.162081\n",
      "Epoch [2/30] Loss: 0.006265 | RMSE: 0.079151\n",
      "Epoch [3/30] Loss: 0.001961 | RMSE: 0.044266\n",
      "Epoch [4/30] Loss: 0.001408 | RMSE: 0.037498\n",
      "Epoch [5/30] Loss: 0.001187 | RMSE: 0.034429\n",
      "Epoch [6/30] Loss: 0.001018 | RMSE: 0.031872\n",
      "Epoch [7/30] Loss: 0.000878 | RMSE: 0.029594\n",
      "Epoch [8/30] Loss: 0.000777 | RMSE: 0.027832\n",
      "Epoch [9/30] Loss: 0.000720 | RMSE: 0.026791\n",
      "Epoch [10/30] Loss: 0.000695 | RMSE: 0.026315\n",
      "Epoch [11/30] Loss: 0.000686 | RMSE: 0.026145\n",
      "Epoch [12/30] Loss: 0.000685 | RMSE: 0.026134\n",
      "Epoch [13/30] Loss: 0.000687 | RMSE: 0.026170\n",
      "Epoch [14/30] Loss: 0.000689 | RMSE: 0.026212\n",
      "Epoch [15/30] Loss: 0.000690 | RMSE: 0.026230\n",
      "Epoch [16/30] Loss: 0.000691 | RMSE: 0.026241\n",
      "Epoch [17/30] Loss: 0.000690 | RMSE: 0.026235\n",
      "Epoch [18/30] Loss: 0.000689 | RMSE: 0.026200\n",
      "Epoch [19/30] Loss: 0.000686 | RMSE: 0.026144\n",
      "Epoch [20/30] Loss: 0.000682 | RMSE: 0.026080\n",
      "Epoch [21/30] Loss: 0.000679 | RMSE: 0.026017\n",
      "Epoch [22/30] Loss: 0.000676 | RMSE: 0.025960\n",
      "Epoch [23/30] Loss: 0.000673 | RMSE: 0.025906\n",
      "Epoch [24/30] Loss: 0.000671 | RMSE: 0.025855\n",
      "Epoch [25/30] Loss: 0.000668 | RMSE: 0.025808\n",
      "Epoch [26/30] Loss: 0.000666 | RMSE: 0.025765\n",
      "Epoch [27/30] Loss: 0.000664 | RMSE: 0.025725\n",
      "Epoch [28/30] Loss: 0.000662 | RMSE: 0.025691\n",
      "Epoch [29/30] Loss: 0.000661 | RMSE: 0.025660\n",
      "Epoch [30/30] Loss: 0.000659 | RMSE: 0.025632\n",
      "Test RMSE: 1.2084 | SMAPE: 8.64%\n",
      "RMSE: 1.2083639757505626\n",
      "\n",
      "===== HPO Trial 2 =====\n",
      "Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 24,\n",
      "    \"hidden_size\": 64,\n",
      "    \"num_layers\": 1,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.001,\n",
      "    \"batch_size\": 32,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adam\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"The dataset has a lag of 24, and increasing hidden size to 64 and reducing dropout from 0.3435927279521027 to 0.15 are likely to improve model capacity while maintaining regularization.\",\n",
      "  \"non_repetition_check\": \"True\",\n",
      "  \"expected_improvement\": \"The combination of these changes is expected to reduce overfitting and increase the model's ability to generalize, potentially leading to a lower RMSE.\"\n",
      "}\n",
      "Reasoning: The dataset has a lag of 24, and increasing hidden size to 64 and reducing dropout from 0.3435927279521027 to 0.15 are likely to improve model capacity while maintaining regularization.\n",
      "Epoch [1/30] Loss: 0.023345 | RMSE: 0.152806\n",
      "Epoch [2/30] Loss: 0.006680 | RMSE: 0.081734\n",
      "Epoch [3/30] Loss: 0.002225 | RMSE: 0.047156\n",
      "Epoch [4/30] Loss: 0.001499 | RMSE: 0.038698\n",
      "Epoch [5/30] Loss: 0.001261 | RMSE: 0.035483\n",
      "Epoch [6/30] Loss: 0.001081 | RMSE: 0.032846\n",
      "Epoch [7/30] Loss: 0.000931 | RMSE: 0.030480\n",
      "Epoch [8/30] Loss: 0.000816 | RMSE: 0.028530\n",
      "Epoch [9/30] Loss: 0.000744 | RMSE: 0.027238\n",
      "Epoch [10/30] Loss: 0.000709 | RMSE: 0.026584\n",
      "Epoch [11/30] Loss: 0.000694 | RMSE: 0.026311\n",
      "Epoch [12/30] Loss: 0.000688 | RMSE: 0.026195\n",
      "Epoch [13/30] Loss: 0.000689 | RMSE: 0.026214\n",
      "Epoch [14/30] Loss: 0.000693 | RMSE: 0.026289\n",
      "Epoch [15/30] Loss: 0.000692 | RMSE: 0.026273\n",
      "Epoch [16/30] Loss: 0.000687 | RMSE: 0.026162\n",
      "Epoch [17/30] Loss: 0.000682 | RMSE: 0.026078\n",
      "Epoch [18/30] Loss: 0.000680 | RMSE: 0.026034\n",
      "Epoch [19/30] Loss: 0.000678 | RMSE: 0.025993\n",
      "Epoch [20/30] Loss: 0.000676 | RMSE: 0.025950\n",
      "Epoch [21/30] Loss: 0.000673 | RMSE: 0.025906\n",
      "Epoch [22/30] Loss: 0.000671 | RMSE: 0.025861\n",
      "Epoch [23/30] Loss: 0.000669 | RMSE: 0.025816\n",
      "Epoch [24/30] Loss: 0.000667 | RMSE: 0.025774\n",
      "Epoch [25/30] Loss: 0.000665 | RMSE: 0.025734\n",
      "Epoch [26/30] Loss: 0.000663 | RMSE: 0.025698\n",
      "Epoch [27/30] Loss: 0.000661 | RMSE: 0.025665\n",
      "Epoch [28/30] Loss: 0.000660 | RMSE: 0.025635\n",
      "Epoch [29/30] Loss: 0.000658 | RMSE: 0.025608\n",
      "Epoch [30/30] Loss: 0.000657 | RMSE: 0.025582\n",
      "Test RMSE: 1.0476 | SMAPE: 8.96%\n",
      "RMSE: 1.0475835575540264\n",
      "Trial recorded ‚Üí C:\\Users\\user.IBRAHIM-IK-SZHE\\Meta_LLM_HPO\\hyperopt-llm-project\\data\\LLM_trials_memory\\trials_history_qwen2.5:3b.json\n",
      "New best RMSE!\n",
      "\n",
      "===== HPO Trial 3 =====\n",
      "Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 24,\n",
      "    \"hidden_size\": 64,\n",
      "    \"num_layers\": 1,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.001,\n",
      "    \"batch_size\": 32,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adam\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"The dataset has a lag of 24, and reducing the hidden size to 64 while keeping other parameters similar is expected to balance model complexity with performance.\",\n",
      "  \"non_repetition_check\": \"True\",\n",
      "  \"expected_improvement\": \"By decreasing dropout from 0.3435927279521027 in the last trial, and reducing epochs from 6 to 30 while keeping other parameters constant, we aim to improve model generalization and reduce overfitting.\"\n",
      "}\n",
      "Reasoning: The dataset has a lag of 24, and reducing the hidden size to 64 while keeping other parameters similar is expected to balance model complexity with performance.\n",
      "Epoch [1/30] Loss: 0.024040 | RMSE: 0.155064\n",
      "Epoch [2/30] Loss: 0.007002 | RMSE: 0.083677\n",
      "Epoch [3/30] Loss: 0.002351 | RMSE: 0.048468\n",
      "Epoch [4/30] Loss: 0.001680 | RMSE: 0.040964\n",
      "Epoch [5/30] Loss: 0.001423 | RMSE: 0.037700\n",
      "Epoch [6/30] Loss: 0.001227 | RMSE: 0.034999\n",
      "Epoch [7/30] Loss: 0.001058 | RMSE: 0.032492\n",
      "Epoch [8/30] Loss: 0.000911 | RMSE: 0.030146\n",
      "Epoch [9/30] Loss: 0.000796 | RMSE: 0.028175\n",
      "Epoch [10/30] Loss: 0.000726 | RMSE: 0.026904\n",
      "Epoch [11/30] Loss: 0.000697 | RMSE: 0.026364\n",
      "Epoch [12/30] Loss: 0.000689 | RMSE: 0.026209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm_opt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhpo_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mhistorical_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43mraw_features_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_rmse\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbest_rmse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/src\\meta_HPO\\meta_llm_tuning.py:220\u001b[39m, in \u001b[36mLLMTuning.hpo_optimization\u001b[39m\u001b[34m(self, n_trials, historical_trials, dataset_sample, raw_features_summary, target_rmse, best_rmse)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReasoning:\u001b[39m\u001b[33m\"\u001b[39m, parsing[\u001b[33m\"\u001b[39m\u001b[33mmeta_feature_reasoning\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m rmse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_bilstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsing\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuggested_params\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRMSE:\u001b[39m\u001b[33m\"\u001b[39m, rmse)\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m#  Update best\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/src\\meta_HPO\\meta_llm_tuning.py:113\u001b[39m, in \u001b[36mLLMTuning.run_bilstm\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_bilstm\u001b[39m(\u001b[38;5;28mself\u001b[39m, params: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs Bi-LSTM and returns RMSE\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBi-LSTM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(results[\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\pipelines\\forecasting_pipeline.py:151\u001b[39m, in \u001b[36mForecastingPipeline.run\u001b[39m\u001b[34m(self, model_name, params, run_suffix)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    150\u001b[39m start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m loss_hist, rmse_hist = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m train_time = time.time() - start\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\models\\training.py:86\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_loader, epochs)\u001b[39m\n\u001b[32m     83\u001b[39m yb = yb.to(\u001b[38;5;28mself\u001b[39m.device).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(preds, yb)\n\u001b[32m     89\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\models\\model.py:31\u001b[39m, in \u001b[36mBiLSTMForecast.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     out = out[:, -\u001b[32m1\u001b[39m, :]            \u001b[38;5;66;03m# (batch, hidden*2)\u001b[39;00m\n\u001b[32m     33\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.fc(out)             \u001b[38;5;66;03m# (batch, horizon)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user.IBRAHIM-IK-SZHE\\Anaconda3\\envs\\HPO311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1089\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1087\u001b[39m unsorted_indices = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     h_zeros = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_directions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreal_hidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m     c_zeros = torch.zeros(\n\u001b[32m   1097\u001b[39m         \u001b[38;5;28mself\u001b[39m.num_layers * num_directions,\n\u001b[32m   1098\u001b[39m         max_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1101\u001b[39m         device=\u001b[38;5;28minput\u001b[39m.device,\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1103\u001b[39m     hx = (h_zeros, c_zeros)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "llm_opt.hpo_optimization(3,historical_trials,dataset_sample,raw_features_summary,target_rmse,best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5e34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPO311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
