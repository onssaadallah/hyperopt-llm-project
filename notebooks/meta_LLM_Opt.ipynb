{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1da8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67583d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (0.14.6)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (1.17.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user.ibrahim-ik-szhe\\anaconda3\\envs\\hpo311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json \n",
    "import time\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sys\n",
    "sys.path.insert(0,r\"C:Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/src\")\n",
    "from data.data_cleaning  import CleanData\n",
    "from data.data_preprocessing import DataPreprocessing\n",
    "from models.model import BiLSTMForecast\n",
    "from models.training import Trainer \n",
    "from pipelines.forecasting_pipeline import ForecastingPipeline\n",
    "import pandas as pd \n",
    "import re\n",
    "import os \n",
    "from meta_HPO.meta_knowledge_building import MetaKnowledgeBuilder\n",
    "from meta_HPO.prompt_template import prompt_template\n",
    "from meta_HPO.LLM_Load import load_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0b432",
   "metadata": {},
   "source": [
    "# 1 .Meta-Knowledge Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9713aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user.IBRAHIM-IK-SZHE\\anaconda3\\envs\\HPO311\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best historical trials:\n",
      "{'run_id': 'trial_1', 'test_rmse': 1.1170297250018493, 'params': {'optimizer': 'adamax', 'epochs': '17', 'num_layers': '1', 'batch_size': '32', 'hidden_size': '126', 'lr': '0.0005182120604022333', 'horizon': '4', 'dropout': '0.0', 'input_dim': '4', 'model_name': 'Bi-LSTM', 'seed': '42', 'lag': '58'}}\n",
      "{'run_id': 'trial_2', 'test_rmse': 1.9278644352798011, 'params': {'optimizer': 'adamw', 'epochs': '6', 'num_layers': '1', 'batch_size': '32', 'hidden_size': '81', 'lr': '0.0003623165345530821', 'horizon': '4', 'dropout': '0.0', 'input_dim': '4', 'model_name': 'Bi-LSTM', 'seed': '42', 'lag': '38'}}\n",
      "{'run_id': 'trial_3', 'test_rmse': 2.7698660920690417, 'params': {'optimizer': 'adagrad', 'epochs': '6', 'num_layers': '3', 'batch_size': '128', 'hidden_size': '77', 'lr': '0.004802430658713304', 'horizon': '4', 'dropout': '0.3435927279521027', 'input_dim': '4', 'model_name': 'Bi-LSTM', 'seed': '42', 'lag': '49'}}\n",
      "‚úÖ Dataset regime:\n",
      "{'temporal_dependence': 'strong', 'stationarity': 'mostly stationary', 'trend': 'moderate', 'seasonality': 'strong', 'volatility': 'moderate', 'noise_level': 'low'}\n",
      "\n",
      "‚úÖ Aggregated meta-features:\n",
      "{'temporal_dependence_mean_acf1': 0.9828705509299869, 'stationarity_ratio': 1.0, 'avg_trend_strength': None, 'avg_seasonality_strength': None, 'avg_volatility_cv': 0.4885260253901742, 'avg_nonlinearity_proxy': 0.7810950068734697, 'avg_outlier_ratio': 0.01757732194252821}\n"
     ]
    }
   ],
   "source": [
    "#-----------test forecasting Pipelien----------\n",
    "# load Clean train/ test data\n",
    "Target_Cols = ['T (degC)', 'rh (%)', 'p (mbar)', 'wv (m/s)']\n",
    "\n",
    "\n",
    "clean_train = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/pape_project/hyperopt-llm-project/data/clean_data/clean_train.csv',\n",
    "parse_dates=[\"datetime\"],\n",
    "index_col=\"datetime\"\n",
    ")\n",
    "\n",
    "# MLflow experiment name (must already exist)\n",
    "EXPERIMENT_NAME = \"Bayesian_Optimization\"\n",
    "meta_builder= MetaKnowledgeBuilder(clean_train=clean_train,experiment_name=EXPERIMENT_NAME,target_cols=Target_Cols)\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "best_trials = meta_builder.create_meta_trials(top_k=top_k)\n",
    "\n",
    "print(\"‚úÖ Best historical trials:\")\n",
    "for t in best_trials:\n",
    "    print(t)\n",
    "\n",
    "meta_features = meta_builder.meta_features_creation()\n",
    "\n",
    "print(\"‚úÖ Dataset regime:\")\n",
    "print(meta_features[\"dataset_regime\"])\n",
    "\n",
    "print(\"\\n‚úÖ Aggregated meta-features:\")\n",
    "print(meta_features[\"aggregated_meta\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d184a",
   "metadata": {},
   "source": [
    "# Meta-knoweldge Extraction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb63ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "#  1.Model Architecture Description\n",
    "# ==================================================\n",
    "model_description = \"\"\"\n",
    "Bidirectional LSTM for multivariate forecasting:\n",
    "- Input: multivariate sequence\n",
    "- hidden_size: LSTM units\n",
    "- num_layers: stacked layers\n",
    "- dropout applied between layers\n",
    "\"\"\"\n",
    "# ==================================================\n",
    "#  1.extract meta-initila trials and meta_knowledge\n",
    "# ==================================================\n",
    "with open(\"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/meta_data/run_data.json\") as f:\n",
    "    best_trials = json.load(f)\n",
    "historical_trials = [t[\"params\"] for t in best_trials]\n",
    "\n",
    "with open(\"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/meta_data/meta_features.json\") as f:\n",
    "    meta_features = json.load(f)\n",
    "\n",
    "raw_features_summary = json.dumps(meta_features[\"raw_meta_features\"], indent=2)\n",
    "summary_meta_fetaures = json.dumps(meta_features[\"dataset_regime\"], indent=2)\n",
    "#--------------------------------\n",
    " # compute Traget Rmse \n",
    "#---------------------------------\n",
    "target_rmse = meta_builder.compute_target_rmse(best_trials, epsilon=0.08)\n",
    "#--------------------------------\n",
    " # Shots id train data \n",
    "#---------------------------------\n",
    "dataset_sample = json.dumps(clean_train.tail(30).to_dict(orient=\"records\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c626c57",
   "metadata": {},
   "source": [
    "# Load Open Source LLM using Ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c211416",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def _ollama_installed():\n",
    "    return shutil.which(\"ollama\") is not None\n",
    "\n",
    "\n",
    "def _ollama_model_exists(model_name: str):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"ollama\", \"list\"], text=True)\n",
    "        return any(model_name in line for line in out.splitlines())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _ollama_pull(model_name: str):\n",
    "    print(f\"‚¨áÔ∏è Pulling Ollama model: {model_name}\")\n",
    "    subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
    "\n",
    "\n",
    "def load_llm(\n",
    "    backend: str = \"ollama\",\n",
    "    model_name: str = \"qwen2:3b\",\n",
    "    temperature: float = 0.2,\n",
    "    warn_large: bool = True\n",
    "):\n",
    "    backend = backend.lower()\n",
    "\n",
    "    # --------------------------\n",
    "    # Ollama (BEST for CPU)\n",
    "    # --------------------------\n",
    "    if backend == \"ollama\":\n",
    "        if not _ollama_installed():\n",
    "            raise RuntimeError(\n",
    "                \"‚ùå Ollama not installed. Install from: https://ollama.com\"\n",
    "            )\n",
    "\n",
    "        if not _ollama_model_exists(model_name):\n",
    "            _ollama_pull(model_name)\n",
    "\n",
    "        if warn_large and any(x in model_name for x in [\"13b\", \"70b\"]):\n",
    "            print(\"‚ö†Ô∏è WARNING: This model may be too large for CPU.\")\n",
    "\n",
    "        from langchain_community.llms import Ollama\n",
    "        return Ollama(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            num_ctx=4096,\n",
    "            num_thread=4   # adjust based on your CPU cores\n",
    "        )\n",
    "\n",
    "    # --------------------------\n",
    "    # HuggingFace (CPU fallback)\n",
    "    # --------------------------\n",
    "    elif backend == \"hf\":\n",
    "        from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "        if warn_large and any(x in model_name.lower() for x in [\"13b\", \"70b\"]):\n",
    "            print(\"‚ö†Ô∏è WARNING: Large HF model on CPU will be VERY slow.\")\n",
    "\n",
    "        return HuggingFaceHub(\n",
    "            repo_id=model_name,\n",
    "            model_kwargs={\n",
    "                \"temperature\": temperature,\n",
    "                \"max_new_tokens\": 512\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backend: ollama | openai | hf\")'''\n",
    "\n",
    "\n",
    "#Load LLama\n",
    "llm = load_llm(\n",
    "    backend=\"ollama\",\n",
    "    model_name=\"llama3:8b\",  # medium-small reasoning model\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9b8cb",
   "metadata": {},
   "source": [
    "# 3. Prompt Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d959d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"model_description\",\n",
    "        \"num_features\",\n",
    "        \"dataset_meta_summary\",\n",
    "        \"raw_features_summary\",\n",
    "        \"dataset_sample\",\n",
    "        \"historical_trials\",\n",
    "        \"target_rmse\",\n",
    "        \"current_best_rmse\"\n",
    "    ],\n",
    "    template=\"\"\"\n",
    "You are an expert hyperparameter optimization agent specialized in\n",
    "MULTIVARIATE TIME SERIES FORECASTING using a BIDIRECTIONAL LSTM (BiLSTM).\n",
    "\n",
    "Your SOLE OBJECTIVE is to MINIMIZE TEST RMSE.\n",
    "You must suggest new configurations that IMPROVE upon the CURRENT BEST RMSE.\n",
    "Treat historical_trials as a TABU LIST ‚Äî NEVER repeat them.\n",
    "\n",
    "==================================================\n",
    "OBJECTIVE\n",
    "==================================================\n",
    "Current best RMSE: {current_best_rmse}\n",
    "Target RMSE: < {target_rmse}\n",
    "Goal: Each suggestion must move the search toward LOWER RMSE.\n",
    "\n",
    "==================================================\n",
    "DATASET CONTEXT & META-FEATURES\n",
    "==================================================\n",
    "Dataset type: Multivariate time series regression\n",
    "Number of input variables: {num_features}\n",
    "\n",
    "Dataset meta-features:\n",
    "{dataset_meta_summary}\n",
    "\n",
    "Use meta-features to reason about hyperparameters:\n",
    "- Seasonality ‚Üí lag\n",
    "- Autocorrelation ‚Üí lag, hidden_size\n",
    "- Cross-variable interactions ‚Üí hidden_size, num_layers\n",
    "- Noise level ‚Üí dropout, batch_size\n",
    "- Stationarity & trend ‚Üí depth, learning rate\n",
    "- Dataset size ‚Üí model capacity, epochs\n",
    "\n",
    "Raw feature summaries (for context only):\n",
    "{raw_features_summary}\n",
    "\n",
    "Small data sample (qualitative insight only ‚Äî do NOT overfit):\n",
    "{dataset_sample}\n",
    "\n",
    "==================================================\n",
    "MODEL CONTEXT\n",
    "==================================================\n",
    "Architecture:\n",
    "- Bidirectional LSTM (capacity is effectively doubled)\n",
    "- Regression output\n",
    "\n",
    "Model description:\n",
    "{model_description}\n",
    "\n",
    "==================================================\n",
    "STRICT SEARCH SPACE\n",
    "==================================================\n",
    "lag ‚àà [12, 16, 24, 32]\n",
    "hidden_size ‚àà [24, 32, 48, 64]\n",
    "num_layers ‚àà [1, 2]\n",
    "dropout ‚àà [0.05, 0.1, 0.15, 0.2]\n",
    "lr ‚àà [0.0005, 0.001, 0.002]\n",
    "batch_size ‚àà [32, 64]\n",
    "epochs ‚àà [20, 30, 40]\n",
    "optimizer ‚àà [\"adam\", \"adamw\"]\n",
    "\n",
    "==================================================\n",
    "HISTORICAL TRIALS (TABU LIST)\n",
    "==================================================\n",
    "{historical_trials}\n",
    "\n",
    "Rules for this trial:\n",
    "1. You MUST NOT repeat any configuration from historical_trials.\n",
    "2. Change AT LEAST TWO hyperparameters from the last trial.\n",
    "3. At least ONE change MUST be from: dropout, lr, or epochs.\n",
    "4. Prioritize changes that are most likely to reduce RMSE based on dataset meta-features.\n",
    "5. Provide concise reasoning for why these changes are expected to improve RMSE.\n",
    "6. Confirm novelty of configuration before output.\n",
    "\n",
    "==================================================\n",
    "OUTPUT FORMAT (STRICT JSON)\n",
    "==================================================\n",
    "Output ONLY valid JSON. No markdown, no comments, no extra text.\n",
    "Return exactly this structure:\n",
    "\n",
    "{{\n",
    "  \"suggested_params\": {{\n",
    "    \"lag\": integer,\n",
    "    \"hidden_size\": integer,\n",
    "    \"num_layers\": integer,\n",
    "    \"dropout\": float,\n",
    "    \"lr\": float,\n",
    "    \"batch_size\": integer,\n",
    "    \"epochs\": integer,\n",
    "    \"optimizer\": \"string\"\n",
    "  }},\n",
    "  \"meta_feature_reasoning\": \"short explanation linking dataset characteristics to hyperparameter choices\",\n",
    "  \"non_repetition_check\": \"confirmation that this configuration is novel and not in historical_trials\",\n",
    "  \"expected_improvement\": \"short estimate of why RMSE should improve\"\n",
    "}}\n",
    "No Extrat Data\n",
    "\"\"\"\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11282f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Pulling Ollama model: phi3:mini\n"
     ]
    }
   ],
   "source": [
    "llm_phi3mini = load_llm(\n",
    "    backend=\"ollama\",\n",
    "    model_name=\"phi3:mini\",\n",
    "    temperature=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284e0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define llm chain \n",
    "suggest_chain = LLMChain(\n",
    "    llm=llm_phi3mini,\n",
    "    prompt=prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce4b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46b7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = \"Bi-LSTM\"\n",
    "experiment_name =\"LLM-AutoOpt\"\n",
    "TARGET_COLS =['T (degC)', 'rh (%)', 'p (mbar)', 'wv (m/s)']\n",
    "clean_train = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_train.csv',\n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "clean_test = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_test.csv', \n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "\n",
    "# define ForecastingPipeline\n",
    "pipeline = ForecastingPipeline(\n",
    "    clean_train=clean_train,\n",
    "    clean_test=clean_test,\n",
    "    target_cols=TARGET_COLS,\n",
    "    experiment_name=experiment_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3528ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tools\n",
    "def run_bilstm(params: dict) -> float:\n",
    "    \"\"\"\n",
    "    Runs Bi-LSTM and returns RMSE\n",
    "    \"\"\"\n",
    "    results = pipeline.run(\"Bi-LSTM\", params)\n",
    "    return float(results[\"rmse\"])\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "#Flexiable \n",
    "def extract_params_and_reasoning(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Flexible extractor for suggested_params and reasoning from LLM output.\n",
    "    Works whether JSON is inside ```json``` fences or plain text.\n",
    "    \"\"\"\n",
    "    # ---------------------------\n",
    "    # 1Ô∏è‚É£ Extract JSON-like substring\n",
    "    # ---------------------------\n",
    "    # Try to find a JSON block inside ```json``` first\n",
    "    json_match = re.search(r\"```json(.*?)```\", text, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1).strip()\n",
    "    else:\n",
    "        # Otherwise, find first {...} block in the text\n",
    "        json_match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No JSON object found in text\")\n",
    "        json_str = json_match.group(0)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2Ô∏è‚É£ Load JSON safely\n",
    "    # ---------------------------\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON detected: {e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3Ô∏è‚É£ Extract keys\n",
    "    # ---------------------------\n",
    "    if \"suggested_params\" not in data:\n",
    "        raise KeyError(\"'suggested_params' not found in JSON\")\n",
    "\n",
    "    reasoning = data.get(\"meta_feature_reasoning\")  # may be None if missing\n",
    "\n",
    "    return {\n",
    "        \"suggested_params\": data[\"suggested_params\"],\n",
    "        \"meta_feature_reasoning\": reasoning\n",
    "    }\n",
    "\n",
    "# record the trials \n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def record_trials(\n",
    "    params,\n",
    "    rmse,\n",
    "    reasoning,\n",
    "     llm_name,\n",
    "    base_dir=\"C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/LLM_trials_memory\",  \n",
    "):\n",
    "    record = {\n",
    "        \"params\": params,\n",
    "        \"rmse\": rmse,\n",
    "        \"reasoning\": reasoning\n",
    "    }\n",
    "    filename = f\"trials_history_{llm_name}.json\"\n",
    "    base_dir = Path(base_dir)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)   # ensure folder exists\n",
    "\n",
    "    file_path = base_dir / filename               # real file path\n",
    "\n",
    "    # Load existing history\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []   # corrupted/empty file fallback\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    # Append new record\n",
    "    data.append(record)\n",
    "\n",
    "    # Save back\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Trial recorded ‚Üí {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b23f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== üöÄ HPO Trial 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user.IBRAHIM-IK-SZHE\\anaconda3\\envs\\HPO311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 16,\n",
      "    \"hidden_size\": 48,\n",
      "    \"num_layers\": 2,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.002,\n",
      "    \"batch_size\": 64,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adamw\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"Increasing the lag to 16 and hidden size to 48 should capture more complex patterns in weather data. A bidirectional LSTM with two layers can better learn from both past and future contexts, which is beneficial for capturing seasonality.\",\n",
      "  \"non_repetition_check\": true,\n",
      "    \"expected_improvement\": \"Based on the dataset's longer-term dependencies and non-stationarity in weather patterns, these changes should enhance learning of temporal features leading to a lower RMSE.\"\n",
      "}\n",
      "Reasoning Increasing the lag to 16 and hidden size to 48 should capture more complex patterns in weather data. A bidirectional LSTM with two layers can better learn from both past and future contexts, which is beneficial for capturing seasonality.\n",
      "Epoch [1/30] Loss: 0.030062 | RMSE: 0.173175\n",
      "Epoch [2/30] Loss: 0.013835 | RMSE: 0.117385\n",
      "Epoch [3/30] Loss: 0.008891 | RMSE: 0.094093\n",
      "Epoch [4/30] Loss: 0.007741 | RMSE: 0.087905\n",
      "Epoch [5/30] Loss: 0.007820 | RMSE: 0.088454\n",
      "Epoch [6/30] Loss: 0.007445 | RMSE: 0.086287\n",
      "Epoch [7/30] Loss: 0.006779 | RMSE: 0.082368\n",
      "Epoch [8/30] Loss: 0.006044 | RMSE: 0.077747\n",
      "Epoch [9/30] Loss: 0.005220 | RMSE: 0.072202\n",
      "Epoch [10/30] Loss: 0.004406 | RMSE: 0.066386\n",
      "Epoch [11/30] Loss: 0.003582 | RMSE: 0.059868\n",
      "Epoch [12/30] Loss: 0.002780 | RMSE: 0.052753\n",
      "Epoch [13/30] Loss: 0.002126 | RMSE: 0.046119\n",
      "Epoch [14/30] Loss: 0.001758 | RMSE: 0.041901\n",
      "Epoch [15/30] Loss: 0.001380 | RMSE: 0.037132\n",
      "Epoch [16/30] Loss: 0.001324 | RMSE: 0.036374\n",
      "Epoch [17/30] Loss: 0.001518 | RMSE: 0.038954\n",
      "Epoch [18/30] Loss: 0.002689 | RMSE: 0.051683\n",
      "Epoch [19/30] Loss: 0.003004 | RMSE: 0.054721\n",
      "Epoch [20/30] Loss: 0.001800 | RMSE: 0.042436\n",
      "Epoch [21/30] Loss: 0.001471 | RMSE: 0.038352\n",
      "Epoch [22/30] Loss: 0.001936 | RMSE: 0.043960\n",
      "Epoch [23/30] Loss: 0.001893 | RMSE: 0.043538\n",
      "Epoch [24/30] Loss: 0.001590 | RMSE: 0.039888\n",
      "Epoch [25/30] Loss: 0.001631 | RMSE: 0.040318\n",
      "Epoch [26/30] Loss: 0.001524 | RMSE: 0.039030\n",
      "Epoch [27/30] Loss: 0.001465 | RMSE: 0.038289\n",
      "Epoch [28/30] Loss: 0.001548 | RMSE: 0.039369\n",
      "Epoch [29/30] Loss: 0.001544 | RMSE: 0.039337\n",
      "Epoch [30/30] Loss: 0.001433 | RMSE: 0.037878\n",
      "Test RMSE: 2.0428 | SMAPE: 13.39%\n",
      "üìâ RMSE: 2.0428047882551583\n",
      "\n",
      "===== üöÄ HPO Trial 2 =====\n",
      "üîß Suggested JSON: {\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 12,\n",
      "    \"hidden_size\": 48,\n",
      "    \"num_layers\": 2,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.002,\n",
      "    \"batch_size\": 64,\n",
      "    \"epochs\": 30,\n",
      "    \"optimizer\": \"adamw\"\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"Increasing the lag to a higher value within our search space could capture more extended temporal dependencies in weather patterns. A larger hidden size and additional LSTM layer should improve model capacity without overfitting, given that we have sufficient data for training.\",\n",
      "  \"non_repetition_check\": true,\n",
      "    \"expected_improvement\": \"The increased lag will allow the network to learn from a broader range of historical weather patterns. The larger hidden size and additional layer should enhance learning capacity while dropout helps prevent overfitting on this specific dataset's complexity.\"\n",
      "}\n",
      "Reasoning Increasing the lag to a higher value within our search space could capture more extended temporal dependencies in weather patterns. A larger hidden size and additional LSTM layer should improve model capacity without overfitting, given that we have sufficient data for training.\n",
      "Epoch [1/30] Loss: 0.030760 | RMSE: 0.175286\n",
      "Epoch [2/30] Loss: 0.015656 | RMSE: 0.124877\n",
      "Epoch [3/30] Loss: 0.009681 | RMSE: 0.098029\n",
      "Epoch [4/30] Loss: 0.007886 | RMSE: 0.088393\n",
      "Epoch [5/30] Loss: 0.007766 | RMSE: 0.087735\n",
      "Epoch [6/30] Loss: 0.007690 | RMSE: 0.087277\n",
      "Epoch [7/30] Loss: 0.007395 | RMSE: 0.085661\n",
      "Epoch [8/30] Loss: 0.006909 | RMSE: 0.082754\n",
      "Epoch [9/30] Loss: 0.006174 | RMSE: 0.078121\n",
      "Epoch [10/30] Loss: 0.005375 | RMSE: 0.072810\n",
      "Epoch [11/30] Loss: 0.004535 | RMSE: 0.066763\n",
      "Epoch [12/30] Loss: 0.003502 | RMSE: 0.058415\n",
      "Epoch [13/30] Loss: 0.002670 | RMSE: 0.050805\n",
      "Epoch [14/30] Loss: 0.001964 | RMSE: 0.043258\n",
      "Epoch [15/30] Loss: 0.001630 | RMSE: 0.039314\n",
      "Epoch [16/30] Loss: 0.001630 | RMSE: 0.039235\n",
      "Epoch [17/30] Loss: 0.001735 | RMSE: 0.040630\n",
      "Epoch [18/30] Loss: 0.002015 | RMSE: 0.043769\n",
      "Epoch [19/30] Loss: 0.002027 | RMSE: 0.044063\n",
      "Epoch [20/30] Loss: 0.001675 | RMSE: 0.039778\n",
      "Epoch [21/30] Loss: 0.001567 | RMSE: 0.038394\n",
      "Epoch [22/30] Loss: 0.001694 | RMSE: 0.039939\n",
      "Epoch [23/30] Loss: 0.001901 | RMSE: 0.042484\n",
      "Epoch [24/30] Loss: 0.001784 | RMSE: 0.041116\n",
      "Epoch [25/30] Loss: 0.001502 | RMSE: 0.037494\n",
      "Epoch [26/30] Loss: 0.001532 | RMSE: 0.038078\n",
      "Epoch [27/30] Loss: 0.001589 | RMSE: 0.038709\n",
      "Epoch [28/30] Loss: 0.001680 | RMSE: 0.039751\n",
      "Epoch [29/30] Loss: 0.001504 | RMSE: 0.037649\n",
      "Epoch [30/30] Loss: 0.001507 | RMSE: 0.037489\n",
      "Test RMSE: 2.5619 | SMAPE: 14.91%\n",
      "üìâ RMSE: 2.5618853489704936\n",
      "\n",
      "===== üöÄ HPO Trial 3 =====\n",
      "üîß Suggested JSON: ```json\n",
      "{\n",
      "  \"suggested_params\": {\n",
      "    \"lag\": 24,\n",
      "    \"hidden_size\": 64,\n",
      "    \"num_layers\": 2,\n",
      "    \"dropout\": 0.15,\n",
      "    \"lr\": 0.002,\n",
      "    \"batch_size\": 64,\n",
      "    \"epochs\": 40,\n",
      "    \"optimizer\": \"adamw\",\n",
      "    \"input_dim\": 4\n",
      "  },\n",
      "  \"meta_feature_reasoning\": \"Increasing the lag to 24 hours and using a larger hidden size of 64 LSTM units may capture more complex temporal dependencies, while increasing epochs should allow for better learning. A higher batch size can improve training stability.\",\n",
      "  \"non_repetition_check\": true,\n",
      "  \"expected_improvement\": \"The increased lag and additional layer are expected to provide the model with a broader context of data which could lead to improved RMSE due to more accurate long-term forecasting. A higher learning rate might accelerate convergence while larger batch size can improve training stability, potentially leading to better performance.\"\n",
      "} \n",
      "```\n",
      "Reasoning Increasing the lag to 24 hours and using a larger hidden size of 64 LSTM units may capture more complex temporal dependencies, while increasing epochs should allow for better learning. A higher batch size can improve training stability.\n",
      "Epoch [1/40] Loss: 0.030730 | RMSE: 0.175322\n",
      "Epoch [2/40] Loss: 0.014605 | RMSE: 0.120866\n",
      "Epoch [3/40] Loss: 0.010022 | RMSE: 0.100122\n",
      "Epoch [4/40] Loss: 0.008484 | RMSE: 0.092117\n",
      "Epoch [5/40] Loss: 0.008300 | RMSE: 0.091112\n",
      "Epoch [6/40] Loss: 0.008253 | RMSE: 0.090854\n",
      "Epoch [7/40] Loss: 0.007911 | RMSE: 0.088950\n",
      "Epoch [8/40] Loss: 0.007399 | RMSE: 0.086021\n",
      "Epoch [9/40] Loss: 0.006740 | RMSE: 0.082101\n",
      "Epoch [10/40] Loss: 0.006112 | RMSE: 0.078186\n",
      "Epoch [11/40] Loss: 0.005380 | RMSE: 0.073353\n",
      "Epoch [12/40] Loss: 0.004513 | RMSE: 0.067184\n",
      "Epoch [13/40] Loss: 0.003555 | RMSE: 0.059620\n",
      "Epoch [14/40] Loss: 0.002641 | RMSE: 0.051385\n",
      "Epoch [15/40] Loss: 0.001998 | RMSE: 0.044690\n",
      "Epoch [16/40] Loss: 0.001501 | RMSE: 0.038728\n",
      "Epoch [17/40] Loss: 0.001167 | RMSE: 0.034152\n",
      "Epoch [18/40] Loss: 0.001019 | RMSE: 0.031902\n",
      "Epoch [19/40] Loss: 0.000971 | RMSE: 0.031144\n",
      "Epoch [20/40] Loss: 0.000964 | RMSE: 0.031036\n",
      "Epoch [21/40] Loss: 0.000925 | RMSE: 0.030394\n",
      "Epoch [22/40] Loss: 0.000953 | RMSE: 0.030853\n",
      "Epoch [23/40] Loss: 0.000887 | RMSE: 0.029768\n",
      "Epoch [24/40] Loss: 0.000959 | RMSE: 0.030952\n",
      "Epoch [25/40] Loss: 0.001016 | RMSE: 0.031859\n",
      "Epoch [26/40] Loss: 0.001290 | RMSE: 0.035911\n",
      "Epoch [27/40] Loss: 0.002613 | RMSE: 0.051116\n",
      "Epoch [28/40] Loss: 0.004391 | RMSE: 0.066265\n",
      "Epoch [29/40] Loss: 0.004981 | RMSE: 0.070581\n",
      "Epoch [30/40] Loss: 0.004292 | RMSE: 0.065508\n",
      "Epoch [31/40] Loss: 0.002370 | RMSE: 0.048675\n",
      "Epoch [32/40] Loss: 0.001620 | RMSE: 0.040232\n",
      "Epoch [33/40] Loss: 0.001230 | RMSE: 0.035056\n",
      "Epoch [34/40] Loss: 0.001107 | RMSE: 0.033255\n",
      "Epoch [35/40] Loss: 0.001168 | RMSE: 0.034158\n",
      "Epoch [36/40] Loss: 0.001371 | RMSE: 0.037014\n",
      "Epoch [37/40] Loss: 0.001784 | RMSE: 0.042223\n",
      "Epoch [38/40] Loss: 0.001767 | RMSE: 0.042017\n",
      "Epoch [39/40] Loss: 0.001619 | RMSE: 0.040220\n",
      "Epoch [40/40] Loss: 0.001502 | RMSE: 0.038734\n",
      "Test RMSE: 2.6661 | SMAPE: 11.01%\n",
      "üìâ RMSE: 2.666056352705715\n"
     ]
    }
   ],
   "source": [
    "  # 1Ô∏è‚É£ Ask LLM for new configuration\n",
    "best_rmse = 1.11\n",
    "\n",
    "max_trials = 3\n",
    "\n",
    "for i in range(max_trials):\n",
    "    print(f\"\\n===== üöÄ HPO Trial {i+1} =====\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Ask LLM for new configuration\n",
    "    config_json = suggest_chain.run(\n",
    "        model_description =  model_description,\n",
    "        num_features = 4,\n",
    "        dataset_meta_summary =  dataset_sample ,\n",
    "        raw_features_summary = raw_features_summary,\n",
    "        dataset_sample = dataset_sample,\n",
    "        historical_trials=json.dumps(historical_trials, indent=2),\n",
    "        current_best_rmse=best_rmse,\n",
    "        target_rmse=target_rmse\n",
    "        )\n",
    "\n",
    "    print(\"üîß Suggested JSON:\", config_json)\n",
    "    parsing=  extract_params_and_reasoning(config_json)\n",
    "    print(\"Reasoning\",parsing[\"meta_feature_reasoning\"])\n",
    "    # 3Ô∏è‚É£ Run model\n",
    "    rmse = run_bilstm(parsing[\"suggested_params\"])\n",
    "    print(\"üìâ RMSE:\", rmse)\n",
    "\n",
    "    # 4Ô∏è‚É£ Update best\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        historical_trials.append(parsing[\"suggested_params\"])\n",
    "        record_trials(parsing[\"suggested_params\"], best_rmse, parsing[\"suggested_params\"])\n",
    "        print(\"‚úÖ New best RMSE!\")\n",
    "\n",
    "    # 5Ô∏è‚É£ Early stopping\n",
    "    if best_rmse <= target_rmse:\n",
    "        print(\"üéØ Target RMSE reached!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e986f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         ID              SIZE      MODIFIED    \n",
      "llama3:8b    365c0bd3c000    4.7 GB    2 hours ago    \n"
     ]
    }
   ],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a21b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen 3B safely via Ollama\n",
    "llm = load_llm(\n",
    "    backend=\"ollama\",\n",
    "    model_name= \"qwen2.5:3b\",  # small CPU-friendly model\n",
    "    temperature=0.15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e3997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define llm chain \n",
    "suggest_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,r\"C:Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/src\")\n",
    "import pandas as pd \n",
    "from tuning.meta_llm_tuning import  LLMTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92b7531",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LLMTuner.__init__() takes 1 positional argument but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      9\u001b[39m clean_test = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mC:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_test.csv\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     10\u001b[39m     parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     11\u001b[39m     index_col=\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m model_description = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mBidirectional LSTM for multivariate forecasting:\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m- Input: multivariate sequence\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33m- dropout applied between layers\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m llm_opt =  \u001b[43mLLMTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclean_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclean_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTARGET_COLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mllm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: LLMTuner.__init__() takes 1 positional argument but 8 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_name =\"qwen2.5:3b\"\n",
    "name_model = \"Bi-LSTM\"\n",
    "experiment_name =\"LLM-AutoOpt\"\n",
    "TARGET_COLS =['T (degC)', 'rh (%)', 'p (mbar)', 'wv (m/s)']\n",
    "clean_train = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_train.csv',\n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "clean_test = pd.read_csv('C:/Users/user.IBRAHIM-IK-SZHE/Meta_LLM_HPO/hyperopt-llm-project/data/clean_data/clean_test.csv', \n",
    "    parse_dates=[\"datetime\"],\n",
    "    index_col=\"datetime\"\n",
    "    )\n",
    "model_description = \"\"\"\n",
    "Bidirectional LSTM for multivariate forecasting:\n",
    "- Input: multivariate sequence\n",
    "- hidden_size: LSTM units\n",
    "- num_layers: stacked layers\n",
    "- dropout applied between layers\n",
    "\"\"\"\n",
    "llm_opt =  LLMTuner(name_model,experiment_name,clean_train,clean_test,TARGET_COLS,llm_name,model_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPO311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
